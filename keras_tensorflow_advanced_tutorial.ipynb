{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatamadavi/Cmpe258-assignment/blob/main/keras_tensorflow_advanced_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "e6f709f7-429f-4499-d5b8-8b03901a0002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      True\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "7d90891a-ee21-481c-a75a-852765423041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "6a2a08e0-07ce-4cb6-aa7b-0a9f2c5efb03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "074e1989-dc9d-4cf6-e203-1c2a6fbf7e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "9e81f1aa-83f0-4e3e-e838-309da6503dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "30200e18-5ebc-4296-a8ed-596a302e12e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "37ff1adc-9841-40da-cd0c-d3f0dadbdc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "0adfa25d-067e-4ab0-d50b-3f831a6c4b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: False\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "733f2d2e-fde8-45dd-8560-759f0f32d1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "289237de-a896-452f-8f5f-47828d81212d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.35754988]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821928 ]\n",
            "\n",
            "Output (training) mean: [-1.4901161e-08  3.1664968e-08 -1.4901161e-08 -7.4505806e-09]\n",
            "Output (training) std:  [0.9999924 0.9999908 0.9999955 0.9999957]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "ed4ee4f0-5239-4309-8de5-bd3066edc038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.41305175  0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744823  -0.47392505  0.6866642  -1.3872215 ]\n",
            "  Output mean: 0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "13331cbb-941c-46d3-a91d-10e425d12e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "65b1ccb6-9e8f-4539-bbcb-c7041e6534df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46475708]\n",
            " [0.         0.         0.8248068 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "0c5b7947-51cb-4846-e14c-d48aea7a5fad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "e71f8895-7244-4d34-eef9-ad398f9c7229"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [1.        1.0000001 1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "9946db50-6d83-4f4b-a312-4bf1bbbadc6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "4ffd75f0-c443-42f2-984c-eef183b24e68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "0b733ae5-99c2-4162-ff0c-a1f069eb66c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "4e0bf991-76b1-453c-820d-13c2a79e4ce0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "cd9d6a1a-eb46-4966-d0d9-9af03ec497a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "0d0b0ecb-86dc-46ff-e998-01da26b73b63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "7da9c3f6-587e-4e09-b67f-3349ca6d4c91"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "09d24540-ef14-4cfb-9501-f573de0accf3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "b476ed46-857b-45ab-9d41-9af50ddd8788"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "48c035e8-b2de-488f-caff-9658a0527d3c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 201ms/step - accuracy: 0.3583 - loss: 1.9889 - val_accuracy: 0.3278 - val_loss: 2.2058\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8869 - loss: 0.8180 - val_accuracy: 0.1278 - val_loss: 2.0632\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9625 - loss: 0.2921 - val_accuracy: 0.1417 - val_loss: 2.0203\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9857 - loss: 0.1344 - val_accuracy: 0.1917 - val_loss: 2.4445\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9900 - loss: 0.0710 - val_accuracy: 0.3028 - val_loss: 1.8272\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0448 - val_accuracy: 0.6056 - val_loss: 1.0053\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0276 - val_accuracy: 0.7861 - val_loss: 0.5888\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0230 - val_accuracy: 0.8333 - val_loss: 0.4130\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0182 - val_accuracy: 0.8889 - val_loss: 0.2979\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9988 - loss: 0.0164 - val_accuracy: 0.8833 - val_loss: 0.5066\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9951 - loss: 0.0512 - val_accuracy: 0.8167 - val_loss: 0.5221\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0255 - val_accuracy: 0.9806 - val_loss: 0.0694\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0117 - val_accuracy: 0.9861 - val_loss: 0.0384\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9889 - val_loss: 0.0324\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.9917 - val_loss: 0.0343\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9889 - val_loss: 0.0291\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9889 - val_loss: 0.0276\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9944 - val_loss: 0.0218\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.9917 - val_loss: 0.0293\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9861 - val_loss: 0.0374\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0086 - val_accuracy: 0.9583 - val_loss: 0.1029\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9969 - loss: 0.0156 - val_accuracy: 0.9806 - val_loss: 0.0766\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0116 - val_accuracy: 0.9694 - val_loss: 0.0971\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0100 - val_accuracy: 0.9778 - val_loss: 0.0695\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9917 - val_loss: 0.0214\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9944 - val_loss: 0.0204\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9917 - val_loss: 0.0201\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0030 - val_accuracy: 0.9889 - val_loss: 0.0346\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9944 - val_loss: 0.0193\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0025 - val_accuracy: 0.9861 - val_loss: 0.0336\n",
            "\n",
            "Test Accuracy: 98.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "809e13f8-ec66-4a55-db60-dbcacad66334"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "f9ec56d0-4337-41fd-e405-1a3b492e15ae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "\n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}