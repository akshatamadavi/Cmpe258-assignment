{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatamadavi/Cmpe258-assignment/blob/main/pytorch_advanced_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsPHwagS130i"
      },
      "source": [
        "# PyTorch Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned PyTorch fundamentals, autograd basics, and the high-level nn.Module API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced Autograd | Nested autograd, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only torch.nn.Parameter |\n",
        "| **IV** | Custom nn.Module Layers | Proper subclassing with forward() |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4YYRNzV130l",
        "outputId": "20988a09-34aa-4a27-8aa8-9c2ec009c429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.10.0+cu128\n",
            "CUDA Available:  True\n",
            "GPU Device:      NVIDIA A100-SXM4-40GB\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "Ready for Advanced PyTorch!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions and GPU\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available:  {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device:      {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced PyTorch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbRlppXM130l"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced Autograd Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used autograd for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested autograd** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** with torch.autograd.Function\n",
        "- **Gradient hooks** for manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUeC9-Kg130m",
        "outputId": "87113769-8707-45b8-c275-b0d18dd44bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# First derivative\n",
        "y = x ** 4\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "\n",
        "# Second derivative (need create_graph=True to continue differentiating)\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
        "\n",
        "# Third derivative\n",
        "d3y_dx3 = torch.autograd.grad(d2y_dx2, x)[0]\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.item()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.item():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.item():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.item():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.item():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CSvHZ-3130m",
        "outputId": "3e68a01c-b941-406c-f009-375c9f2e16f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1.0, 2.0, 3.0]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.0, 2.0, 0.14112000167369843]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "def f(x):\n",
        "    return torch.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        torch.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian using torch.autograd.functional.jacobian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "\n",
        "print(f\"\\nx = {x.tolist()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {f(x).tolist()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmVwG5QM130m",
        "outputId": "45ebbf39-496e-4ab2-ce4f-a63453041e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [4.0, 13.0]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "# Scalar function: f(x, y) = x^2*y + y^3\n",
        "def scalar_fn(x):\n",
        "    return x[0]**2 * x[1] + x[1]**3\n",
        "\n",
        "# Compute Hessian using torch.autograd.functional.hessian\n",
        "hessian = torch.autograd.functional.hessian(scalar_fn, x)\n",
        "\n",
        "# Also compute gradient for reference\n",
        "f_val = scalar_fn(x)\n",
        "grad = torch.autograd.grad(f_val, x, create_graph=True)[0]\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].item()}, {x[1].item()})\")\n",
        "print(f\"f = {f_val.item()}\")\n",
        "print(f\"\\nGradient: {grad.tolist()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1mZBZRH130m",
        "outputId": "43f3ca30-dc36-4e70-d1ed-28e1ee07aad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CUSTOM AUTOGRAD FUNCTIONS\n",
            "============================================================\n",
            "\n",
            "Input: [3.0, 4.0]\n",
            "Gradient (clipped to norm 1.0): [0.7071067690849304, 0.7071067690849304]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM AUTOGRAD FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CUSTOM AUTOGRAD FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# torch.autograd.Function allows you to define custom forward and backward passes\n",
        "# This is the PyTorch equivalent of TensorFlow's @tf.custom_gradient\n",
        "\n",
        "class ClipGradientNorm(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, clip_value):\n",
        "        # Save clip_value for backward\n",
        "        ctx.clip_value = clip_value\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Clip the incoming gradient\n",
        "        norm = grad_output.norm()\n",
        "        if norm > ctx.clip_value:\n",
        "            grad_output = grad_output * ctx.clip_value / norm\n",
        "        # Return gradients for x and None for clip_value (not differentiable)\n",
        "        return grad_output, None\n",
        "\n",
        "# Convenience function\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    return ClipGradientNorm.apply(x, clip_value)\n",
        "\n",
        "# Test custom gradient\n",
        "x = torch.tensor([3.0, 4.0], requires_grad=True)  # Gradient will have norm 5\n",
        "\n",
        "y = clip_gradient_norm(x, clip_value=1.0)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput: {x.tolist()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {x.grad.tolist()}\")\n",
        "print(f\"Gradient norm: {x.grad.norm().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIdVGsk130n",
        "outputId": "0d258450-2ea6-47cc-e468-746e11f2262b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.30000001192092896, 0.699999988079071, 1.2000000476837158, 2.5]\n",
            "Rounded: [0.0, 1.0, 1.0, 2.0]\n",
            "Gradient (straight-through): [0.0, 2.0, 2.0, 4.0]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "class StraightThroughRound(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        return torch.round(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output  # Straight-through: gradient = identity\n",
        "\n",
        "class StraightThroughSign(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return torch.sign(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return grad_output * (x.abs() <= 1).float()\n",
        "\n",
        "# Convenience functions\n",
        "straight_through_round = StraightThroughRound.apply\n",
        "straight_through_sign = StraightThroughSign.apply\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([0.3, 0.7, 1.2, 2.5], requires_grad=True)\n",
        "\n",
        "y = straight_through_round(x)\n",
        "loss = (y ** 2).sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput:   {x.tolist()}\")\n",
        "print(f\"Rounded: {y.tolist()}\")\n",
        "print(f\"Gradient (straight-through): {x.grad.tolist()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAQA7S-x130n",
        "outputId": "e83ee264-73c5-4d98-cdef-300ff93fce87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "               GRADIENT HOOKS\n",
            "============================================================\n",
            "\n",
            "Gradient logged:\n",
            "  Mean: 0.3841\n",
            "  Std:  1.7298\n",
            "  Norm: 5.3297\n",
            "\n",
            " Hooks are powerful for:\n",
            "  - Debugging gradient flow\n",
            "  - Gradient clipping per-tensor\n",
            "  - Feature visualization (GradCAM)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT HOOKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"               GRADIENT HOOKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hooks allow you to inspect or modify gradients during backward pass\n",
        "# register_hook() on tensors, register_backward_hook() on modules\n",
        "\n",
        "gradient_log = []\n",
        "\n",
        "def gradient_logger_hook(grad):\n",
        "    \"\"\"Hook that logs gradient statistics.\"\"\"\n",
        "    gradient_log.append({\n",
        "        'mean': grad.mean().item(),\n",
        "        'std': grad.std().item(),\n",
        "        'norm': grad.norm().item()\n",
        "    })\n",
        "    return grad  # Return unchanged gradient\n",
        "\n",
        "def gradient_clip_hook(max_norm):\n",
        "    \"\"\"Create a hook that clips gradient norm.\"\"\"\n",
        "    def hook(grad):\n",
        "        norm = grad.norm()\n",
        "        if norm > max_norm:\n",
        "            return grad * max_norm / norm\n",
        "        return grad\n",
        "    return hook\n",
        "\n",
        "# Example: Log gradients during training\n",
        "x = torch.randn(10, requires_grad=True)\n",
        "handle = x.register_hook(gradient_logger_hook)\n",
        "\n",
        "# Forward and backward\n",
        "y = (x ** 2).sum()\n",
        "y.backward()\n",
        "\n",
        "print(f\"\\nGradient logged:\")\n",
        "print(f\"  Mean: {gradient_log[-1]['mean']:.4f}\")\n",
        "print(f\"  Std:  {gradient_log[-1]['std']:.4f}\")\n",
        "print(f\"  Norm: {gradient_log[-1]['norm']:.4f}\")\n",
        "\n",
        "# Remove hook when done\n",
        "handle.remove()\n",
        "\n",
        "print(\"\\n Hooks are powerful for:\")\n",
        "print(\"  - Debugging gradient flow\")\n",
        "print(\"  - Gradient clipping per-tensor\")\n",
        "print(\"  - Feature visualization (GradCAM)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh3MJQUZ130n",
        "outputId": "9f63eeff-b983-4c9b-8924-e9db1f21cd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. optimizer.zero_grad() once at start\n",
            "  2. Scale loss by 1/accumulation_steps\n",
            "  3. loss.backward() accumulates gradients\n",
            "  4. optimizer.step() after all steps\n",
            "  5. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer, loss_fn):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    model.train()\n",
        "\n",
        "    # Zero gradients once at the start\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(x_batch)\n",
        "        loss = loss_fn(predictions, y_batch)\n",
        "\n",
        "        # Scale loss by accumulation steps (to average gradients)\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass - gradients accumulate!\n",
        "        loss.backward()\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item() * accumulation_steps  # Return unscaled loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. optimizer.zero_grad() once at start\")\n",
        "print(\"  2. Scale loss by 1/accumulation_steps\")\n",
        "print(\"  3. loss.backward() accumulates gradients\")\n",
        "print(\"  4. optimizer.step() after all steps\")\n",
        "print(\"  5. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFNAZ2Z130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using nn.Module layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW8lDgQj130o",
        "outputId": "55b2bf2d-e03f-42b5-e482-32595601f440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  torch.Size([1, 1, 5, 5])\n",
            "Kernel shape: torch.Size([2, 1, 3, 3])\n",
            "Output shape: torch.Size([1, 2, 3, 3])\n",
            "Matches F.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, in_channels, height, width)\n",
        "    kernel : tensor (out_channels, in_channels, kernel_h, kernel_w)\n",
        "    stride : int\n",
        "    padding : int\n",
        "    \"\"\"\n",
        "    batch_size, in_channels, in_h, in_w = input_tensor.shape\n",
        "    out_channels, _, k_h, k_w = kernel.shape\n",
        "\n",
        "    # Apply padding\n",
        "    if padding > 0:\n",
        "        input_tensor = F.pad(input_tensor, [padding] * 4)\n",
        "        in_h += 2 * padding\n",
        "        in_w += 2 * padding\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, out_channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract patch: (batch, in_channels, k_h, k_w)\n",
        "            patch = input_tensor[:, :, h_start:h_start+k_h, w_start:w_start+k_w]\n",
        "\n",
        "            # Convolve: sum over (in_channels, k_h, k_w)\n",
        "            # patch: (batch, in_c, k_h, k_w)\n",
        "            # kernel: (out_c, in_c, k_h, k_w)\n",
        "            # output: (batch, out_c)\n",
        "            conv = torch.einsum('bihw,oihw->bo', patch, kernel)\n",
        "            output[:, :, i, j] = conv\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = torch.randn(1, 1, 5, 5)  # 1 image, 1 channel, 5x5\n",
        "kernel = torch.randn(2, 1, 3, 3)  # 2 output channels, 1 input channel, 3x3\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding=0)\n",
        "torch_output = F.conv2d(x, kernel, stride=1, padding=0)\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches F.conv2d: {torch.allclose(our_output, torch_output, atol=1e-5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoqcFXo3130o",
        "outputId": "00d37e29-eec1-4c69-b12a-51f29cce38b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 1, 4, 4])\n",
            "Input:\n",
            "[[ 1.  2.  3.  4.]\n",
            " [ 5.  6.  7.  8.]\n",
            " [ 9. 10. 11. 12.]\n",
            " [13. 14. 15. 16.]]\n",
            "\n",
            "Output shape: torch.Size([1, 1, 2, 2])\n",
            "Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Matches F.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size, channels, in_h, in_w = input_tensor.shape\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, :, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size]\n",
        "            # Max over spatial dimensions\n",
        "            output[:, :, i, j] = window.amax(dim=(2, 3))\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([[[[1., 2., 3., 4.],\n",
        "                    [5., 6., 7., 8.],\n",
        "                    [9., 10., 11., 12.],\n",
        "                    [13., 14., 15., 16.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input:\")\n",
        "print(x[0, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "torch_pool = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output:\")\n",
        "print(our_pool[0, 0].numpy())\n",
        "print(f\"\\nMatches F.max_pool2d: {torch.allclose(our_pool, torch_pool)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLm27c7q130o",
        "outputId": "7dde3d9d-b30b-4616-c5db-0e0d1bc2ab47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([8, 4])\n",
            "Input mean per feature: [0.5206222534179688, -0.17970936000347137, 0.45833995938301086, 0.2649056315422058]\n",
            "Input std per feature:  [0.3801516592502594, 1.219132661819458, 0.4272221326828003, 0.585858941078186]\n",
            "\n",
            "Output (training) mean: ['-0.0000', '0.0000', '0.0000', '0.0000']\n",
            "Output (training) std:  ['1.0690', '1.0690', '1.0690', '1.0690']\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
        "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = torch.zeros(num_features)\n",
        "        self.running_var = torch.ones(num_features)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = x.mean(dim=0)\n",
        "            batch_var = x.var(dim=0, unbiased=False)\n",
        "\n",
        "            # Update running statistics (detached, no gradient)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = torch.randn(8, 4)  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {x.mean(dim=0).tolist()}\")\n",
        "print(f\"Input std per feature:  {x.std(dim=0).tolist()}\")\n",
        "print(f\"\\nOutput (training) mean: {[f'{v:.4f}' for v in y_train.mean(dim=0).tolist()]}\")\n",
        "print(f\"Output (training) std:  {[f'{v:.4f}' for v in y_train.std(dim=0).tolist()]}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0QylO2n130o",
        "outputId": "3115cea1-5365-4198-d350-51cbdb4a1bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([2, 3, 4])\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [-0.16911523044109344, 1.931160569190979, 1.0118638277053833, -1.4364064931869507]\n",
            "  Output: ['-0.3981', '1.2626', '0.5357', '-1.4002']\n",
            "  Output mean: 0.000000\n",
            "  Output std:  1.1547\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(normalized_shape, requires_grad=True)\n",
        "        self.beta = torch.zeros(normalized_shape, requires_grad=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = torch.randn(2, 3, 4)  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].tolist()}\")\n",
        "print(f\"  Output: {[f'{v:.4f}' for v in y[0, 0, :].tolist()]}\")\n",
        "print(f\"  Output mean: {y[0, 0, :].mean().item():.6f}\")\n",
        "print(f\"  Output std:  {y[0, 0, :].std().item():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlXqtm1V130o",
        "outputId": "3828f2b7-075b-41fd-ae0d-1e4545626840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape torch.Size([2, 10])\n",
            "\n",
            "Dropout sample 1: [0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0]\n",
            "Dropout sample 2: [2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0]\n",
            "Dropout sample 3: [2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "\n",
            "Average over 1000 samples: 1.0055 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = (torch.rand_like(x) < keep_prob).float()\n",
        "\n",
        "    # Apply mask and scale (inverted dropout)\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = torch.ones(2, 10)\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].tolist()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].tolist()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = torch.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {samples.mean().item():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eXTxywF130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with nn.Parameter Only\n",
        "\n",
        "Before using PyTorch's layer classes, let's build fully functional layers using only basic operations. This shows exactly what happens under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVIR_S55130o",
        "outputId": "adbac677-5172-4ed5-926f-e02950a3b3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 3])\n",
            "Weight shape: torch.Size([4, 3])\n",
            "Bias shape:   torch.Size([3])\n",
            "\n",
            "Output:\n",
            "[[2.5953684  0.         2.304854  ]\n",
            " [0.         0.66387445 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only basic PyTorch operations.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "            'softmax': lambda x: F.softmax(x, dim=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = torch.randn(in_features, out_features, requires_grad=True) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = torch.zeros(out_features, requires_grad=True)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = torch.randn(2, 4)\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.detach().numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM5Lo_Ci130p",
        "outputId": "4f699958-c9e8-4a5e-89e6-3841d2d6e972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\n",
            "Input shape:  torch.Size([1, 3, 28, 28])\n",
            "Output shape: torch.Size([1, 16, 28, 28])\n",
            "Kernel shape: torch.Size([16, 3, 3, 3])\n",
            "Parameters:   448\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only basic operations and F.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (out_channels, in_channels, height, width)\n",
        "        self.kernel = torch.randn(\n",
        "            out_channels, in_channels, kernel_size[0], kernel_size[1],\n",
        "            requires_grad=True\n",
        "        ) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = torch.zeros(out_channels, requires_grad=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using F.conv2d\"\"\"\n",
        "        out = F.conv2d(x, self.kernel, bias=self.bias,\n",
        "                       stride=self.stride, padding=self.padding)\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1, activation='relu')\n",
        "x = torch.randn(1, 3, 28, 28)  # 1 image, 3 channels (RGB), 28x28\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {conv.kernel.numel() + conv.bias.numel():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz6Uihdn130p",
        "outputId": "7dea2d4d-a4ad-4ddc-bf38-02c478e8bba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  torch.Size([4, 1, 28, 28])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Output sum per sample: [1.0, 1.0, 0.9999999403953552, 0.9999998807907104]  (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[0]  # PyTorch: (C, H, W)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, padding=1, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, padding=1, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With same padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[1] // 4, input_shape[2] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.parameters())\n",
        "        return params\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(p.numel() for p in layer.parameters())\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(1, 28, 28), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(4, 1, 28, 28)\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {y.sum(dim=1).tolist()}  (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ7pcu0G130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom nn.Module Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "PyTorch's `nn.Module` provides a clean API for custom layers with:\n",
        "- **Automatic parameter registration** with nn.Parameter\n",
        "- **forward() method** for the forward pass\n",
        "- **Proper device management** with `.to(device)`\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking via `.parameters()`\n",
        "- Serialization with `state_dict()`\n",
        "- Integration with optimizers and training loops\n",
        "- Proper shape inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RoTZXGj130p",
        "outputId": "8c02b389-d342-40ed-c346-6437877958cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM nn.Module LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(16, 32, activation='relu')\n",
            "Input shape:  torch.Size([4, 16])\n",
            "Output shape: torch.Size([4, 32])\n",
            "Weight shape: torch.Size([16, 32])\n",
            "Trainable parameters: 544\n",
            "\n",
            "Model:\n",
            "CustomDenseLayer(\n",
            "  in_features=16, out_features=32, bias=True\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM nn.Module LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM nn.Module LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the nn.Module API.\n",
        "\n",
        "    Key points:\n",
        "    - Inherit from nn.Module\n",
        "    - Call super().__init__() in __init__\n",
        "    - Use nn.Parameter for learnable weights\n",
        "    - Implement forward() method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # nn.Parameter automatically registers weights!\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Store activation\n",
        "        self.activation = {\n",
        "            None: nn.Identity(),\n",
        "            'relu': nn.ReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }.get(activation, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return self.activation(output)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        \"\"\"Extra info for print(model).\"\"\"\n",
        "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(16, 32, activation='relu')\n",
        "x = torch.randn(4, 16)\n",
        "y = custom_dense(x)\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(16, 32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {custom_dense.weight.shape}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in custom_dense.parameters())}\")\n",
        "print(f\"\\nModel:\")\n",
        "print(custom_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xlc5mqo130p",
        "outputId": "43c09ef8-7a3c-46c6-fcfb-7a023074e680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:             torch.Size([2, 10, 64])\n",
            "Output shape:            torch.Size([2, 10, 64])\n",
            "Attention weights shape: torch.Size([2, 4, 10, 10])\n",
            "Trainable parameters:    16,384\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x)  # (batch, seq, embed)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # Now: (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = torch.randn(2, 10, 64)  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:             {x.shape}\")\n",
        "print(f\"Output shape:            {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:    {sum(p.numel() for p in attention.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOwGtJYQ130p",
        "outputId": "f43d3e97-284a-4b59-eaee-6c9717c9d7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNorm(32, 64)\n",
            "Input shape:  torch.Size([4, 32])\n",
            "Output shape: torch.Size([4, 64])\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for linear layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, power_iterations=1):\n",
        "        super().__init__()\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "        # Weight matrix\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        # Register buffers for u and v vectors (not parameters, but saved in state_dict)\n",
        "        self.register_buffer('u', torch.randn(out_features))\n",
        "        self.register_buffer('v', torch.randn(in_features))\n",
        "\n",
        "    def _power_iteration(self):\n",
        "        \"\"\"Estimate largest singular value using power iteration.\"\"\"\n",
        "        u = self.u\n",
        "        v = self.v\n",
        "\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = F.normalize(self.weight.t() @ u, dim=0)\n",
        "            # u = W v / ||W v||\n",
        "            u = F.normalize(self.weight @ v, dim=0)\n",
        "\n",
        "        # Update buffers (detached)\n",
        "        self.u.copy_(u.detach())\n",
        "        self.v.copy_(v.detach())\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        sigma = torch.dot(u, self.weight @ v)\n",
        "        return sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = self._power_iteration()\n",
        "        weight_normalized = self.weight / sigma\n",
        "        return F.linear(x, weight_normalized, self.bias)\n",
        "\n",
        "# Test\n",
        "spectral_layer = SpectralNorm(32, 64)\n",
        "x = torch.randn(4, 32)\n",
        "y = spectral_layer(x)\n",
        "\n",
        "print(f\"\\nSpectralNorm(32, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQJeSE01130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQuurStD130p",
        "outputId": "fe57c8fc-2fb7-480f-b8fd-ea5cb12450c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64, 64)\n",
            "Input shape:  torch.Size([2, 64, 32, 32])\n",
            "Output shape: torch.Size([2, 64, 32, 32])\n",
            "\n",
            "ResidualBlock(64, 128, stride=2)\n",
            "Output shape: torch.Size([2, 128, 16, 16])  (spatial dims halved, channels doubled)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main path\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Skip connection + activation\n",
        "        return F.relu(out + self.skip(x))\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, 64, stride=1)\n",
        "x = torch.randn(2, 64, 32, 32)\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(64, 128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(64, 128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1DJ8LX130p",
        "outputId": "494401ae-499c-4dc1-e7d7-1c7d890e3e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
            "Input shape:  torch.Size([2, 64, 28, 28])\n",
            "Output shape: torch.Size([2, 64, 28, 28])\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (B,C,H,W) -> (B,C,1,1)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        reduced_channels = max(channels // reduction_ratio, 1)\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, reduced_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(reduced_channels, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, _, _ = x.shape\n",
        "\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = self.squeeze(x).view(batch_size, channels)\n",
        "\n",
        "        # Excitation: Learn channel importance\n",
        "        attention = self.excitation(squeezed).view(batch_size, channels, 1, 1)\n",
        "\n",
        "        # Scale: Apply attention\n",
        "        return x * attention\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
        "x = torch.randn(2, 64, 28, 28)\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(channels=64, reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrWgtJfV130p",
        "outputId": "cd473322-168d-4461-b413-a9fe690cbea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  torch.Size([2, 20, 64])\n",
            "Output shape: torch.Size([2, 20, 64])\n",
            "Parameters:   49,984\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture (Pre-Norm):\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        normed = self.norm1(x)\n",
        "        attn_output, _ = self.attention(normed, normed, normed, attn_mask=mask)\n",
        "        x = x + self.dropout(attn_output)  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        normed = self.norm2(x)\n",
        "        ff_output = self.ffn(normed)\n",
        "        x = x + ff_output  # Residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 20, 64)  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(p.numel() for p in transformer_block.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmeC-7iT130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control Over Training\n",
        "\n",
        "While high-level training abstractions are convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kteN0deX130p",
        "outputId": "d99a78a7-0dc2-4f86-fb31-9b883a694d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. model.train() - enables dropout, batch norm training mode\n",
            "  2. optimizer.zero_grad() - clears old gradients\n",
            "  3. loss.backward() - computes gradients\n",
            "  4. optimizer.step() - updates weights\n",
            "  5. model.eval() + torch.no_grad() - for validation\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    learning_rate=0.001,\n",
        "    device='cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete custom training loop.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAINING PHASE -----\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            train_total += x_batch.size(0)\n",
        "\n",
        "        # ----- VALIDATION PHASE -----\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "                val_loss += loss.item() * x_batch.size(0)\n",
        "                val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "                val_total += x_batch.size(0)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss / train_total)\n",
        "        history['train_acc'].append(train_correct / train_total)\n",
        "        history['val_loss'].append(val_loss / val_total)\n",
        "        history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % max(1, epochs // 10) == 0:\n",
        "            print(f\"Epoch {epoch+1:4d}/{epochs} | \"\n",
        "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "                  f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "                  f\"Val Loss: {history['val_loss'][-1]:.4f} | \"\n",
        "                  f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. model.train() - enables dropout, batch norm training mode\")\n",
        "print(\"  2. optimizer.zero_grad() - clears old gradients\")\n",
        "print(\"  3. loss.backward() - computes gradients\")\n",
        "print(\"  4. optimizer.step() - updates weights\")\n",
        "print(\"  5. model.eval() + torch.no_grad() - for validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGa8i9A130p",
        "outputId": "8671f957-469f-4524-e65f-597d5b031f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options in PyTorch:\n",
            "  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\n",
            "  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = loss_fn(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), total_norm.item()\n",
        "\n",
        "print(\"Gradient Clipping Options in PyTorch:\")\n",
        "print(\"  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\")\n",
        "print(\"  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWvo8bzn130q",
        "outputId": "62893688-c1cf-442a-cef0-b8156f26191c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LEARNING RATE SCHEDULING\n",
            "============================================================\n",
            "\n",
            "Available LR Schedulers:\n",
            "  - StepLR\n",
            "  - ExponentialLR\n",
            "  - CosineAnnealingLR\n",
            "  - ReduceLROnPlateau\n",
            "  - OneCycleLR\n",
            "\n",
            "Usage in training loop:\n",
            "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
            "  for epoch in range(epochs):\n",
            "      train_one_epoch(...)\n",
            "      scheduler.step()  # Update learning rate\n",
            "\n",
            "Linear Warmup Pattern:\n",
            "  warmup_steps = 1000\n",
            "  for step in range(warmup_steps):\n",
            "      lr = base_lr * (step / warmup_steps)\n",
            "      for param_group in optimizer.param_groups:\n",
            "          param_group['lr'] = lr\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#            LEARNING RATE SCHEDULING AND WARMUP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LEARNING RATE SCHEDULING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# PyTorch provides many schedulers!\n",
        "model = nn.Linear(10, 2)  # Dummy model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Common schedulers\n",
        "schedulers = {\n",
        "    'StepLR': torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),\n",
        "    'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
        "    'CosineAnnealingLR': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50),\n",
        "    'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5),\n",
        "    'OneCycleLR': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=100),\n",
        "}\n",
        "\n",
        "print(\"\\nAvailable LR Schedulers:\")\n",
        "for name in schedulers:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "print(\"\\nUsage in training loop:\")\n",
        "print(\"  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\")\n",
        "print(\"  for epoch in range(epochs):\")\n",
        "print(\"      train_one_epoch(...)\")\n",
        "print(\"      scheduler.step()  # Update learning rate\")\n",
        "\n",
        "# Warmup example\n",
        "print(\"\\nLinear Warmup Pattern:\")\n",
        "print(\"  warmup_steps = 1000\")\n",
        "print(\"  for step in range(warmup_steps):\")\n",
        "print(\"      lr = base_lr * (step / warmup_steps)\")\n",
        "print(\"      for param_group in optimizer.param_groups:\")\n",
        "print(\"          param_group['lr'] = lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5E8_Cmh130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in a real example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6i9U7z-130q",
        "outputId": "a78caccc-afdc-4fd8-eb64-12c9ec77b8ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      torch.Size([1, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#              DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 1, 8, 8)\n",
        "X = X.reshape(-1, 1, 8, 8).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqoufajq130q",
        "outputId": "5a20d83c-14ea-4822-b770-5af2ae9eb300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MiniResNet Architecture:\n",
            "MiniResNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (res_block1): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Identity()\n",
            "  )\n",
            "  (res_block2): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (se_block): SqueezeExcitationBlock(\n",
            "    (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
            "    (excitation): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (3): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 78,418\n"
          ]
        }
      ],
      "source": [
        "# Build custom ResNet model\n",
        "class MiniResNet(nn.Module):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32, 32)\n",
        "        self.res_block2 = ResidualBlock(32, 64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(64, reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Residual blocks\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "\n",
        "        # SE attention\n",
        "        x = self.se_block(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "torch.manual_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10).to(device)\n",
        "\n",
        "print(\"\\nMiniResNet Architecture:\")\n",
        "print(resnet_model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKewny5v130q",
        "outputId": "43baaa17-8d46-46e5-8395-34e691d7ddea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "\n",
            "Epoch  5/30 | Train Loss: 0.0733 | Train Acc: 0.9951 | Val Acc: 0.9861\n",
            "Epoch 10/30 | Train Loss: 0.0242 | Train Acc: 0.9993 | Val Acc: 0.9944\n",
            "Epoch 15/30 | Train Loss: 0.0086 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 20/30 | Train Loss: 0.0061 | Train Acc: 1.0000 | Val Acc: 0.9917\n",
            "Epoch 25/30 | Train Loss: 0.0059 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 30/30 | Train Loss: 0.0046 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "\n",
            "Final Test Accuracy: 99.44%\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nTraining MiniResNet...\\n\")\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(30):\n",
        "    # Training\n",
        "    resnet_model.train()\n",
        "    train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_model(x_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "        train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "        train_total += x_batch.size(0)\n",
        "\n",
        "    # Validation\n",
        "    resnet_model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = resnet_model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "            val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            val_total += x_batch.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Record\n",
        "    history['train_loss'].append(train_loss / train_total)\n",
        "    history['train_acc'].append(train_correct / train_total)\n",
        "    history['val_loss'].append(val_loss / val_total)\n",
        "    history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:2d}/30 | \"\n",
        "              f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "              f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "              f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {history['val_acc'][-1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8iNpgt2m130q",
        "outputId": "96aeb2ad-7bde-47b1-c620-26eba59ff60a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history['train_acc'], 'b-', label='Train Acc')\n",
        "ax2.plot(history['val_acc'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "resnet_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_sample = X_test[:10].to(device)\n",
        "    predictions = resnet_model(X_sample)\n",
        "    probs = F.softmax(predictions, dim=1)\n",
        "    pred_classes = predictions.argmax(dim=1).cpu()\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i, 0].numpy()\n",
        "    true_label = y_test[i].item()\n",
        "    pred_label = pred_classes[i].item()\n",
        "    confidence = probs[i, pred_label].item() * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJz3WzGk130q",
        "outputId": "36100a05-9c9c-41b4-9fdc-ad65bc9430b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                 ADVANCED PYTORCH CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "AUTOGRAD PATTERNS\n",
            "-----------------\n",
            "# Basic gradient\n",
            "y = model(x)\n",
            "y.backward()\n",
            "grads = [p.grad for p in model.parameters()]\n",
            "\n",
            "# Higher-order derivatives\n",
            "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
            "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
            "\n",
            "# Jacobian and Hessian\n",
            "jacobian = torch.autograd.functional.jacobian(f, x)\n",
            "hessian = torch.autograd.functional.hessian(f, x)\n",
            "\n",
            "# Custom autograd function\n",
            "class CustomOp(torch.autograd.Function):\n",
            "    @staticmethod\n",
            "    def forward(ctx, x):\n",
            "        ctx.save_for_backward(x)\n",
            "        return forward_result\n",
            "    @staticmethod\n",
            "    def backward(ctx, grad_output):\n",
            "        x, = ctx.saved_tensors\n",
            "        return custom_gradient\n",
            "\n",
            "CUSTOM nn.Module LAYERS\n",
            "-----------------------\n",
            "class CustomLayer(nn.Module):\n",
            "    def __init__(self, in_features, out_features):\n",
            "        super().__init__()\n",
            "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
            "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
            "\n",
            "    def forward(self, x):\n",
            "        return x @ self.weight + self.bias\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Training loop\n",
            "model.train()\n",
            "for x, y in train_loader:\n",
            "    optimizer.zero_grad()\n",
            "    loss = loss_fn(model(x), y)\n",
            "    loss.backward()\n",
            "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "    optimizer.step()\n",
            "scheduler.step()\n",
            "\n",
            "# Validation loop\n",
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    for x, y in val_loader:\n",
            "        outputs = model(x)\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "for i, (x, y) in enumerate(loader):\n",
            "    loss = loss_fn(model(x), y) / accumulation_steps\n",
            "    loss.backward()\n",
            "    if (i + 1) % accumulation_steps == 0:\n",
            "        optimizer.step()\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "# Gradient hooks\n",
            "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                 ADVANCED PYTORCH CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "AUTOGRAD PATTERNS\n",
        "-----------------\n",
        "# Basic gradient\n",
        "y = model(x)\n",
        "y.backward()\n",
        "grads = [p.grad for p in model.parameters()]\n",
        "\n",
        "# Higher-order derivatives\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
        "\n",
        "# Jacobian and Hessian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "hessian = torch.autograd.functional.hessian(f, x)\n",
        "\n",
        "# Custom autograd function\n",
        "class CustomOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return forward_result\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        return custom_gradient\n",
        "\n",
        "CUSTOM nn.Module LAYERS\n",
        "-----------------------\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Training loop\n",
        "model.train()\n",
        "for x, y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(model(x), y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "scheduler.step()\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        outputs = model(x)\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "for i, (x, y) in enumerate(loader):\n",
        "    loss = loss_fn(model(x), y) / accumulation_steps\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Gradient hooks\n",
        "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jguqJC8p130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced PyTorch Journey\n",
        "\n",
        "Congratulations! You've mastered advanced PyTorch techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced Autograd | Nested autograd, Jacobians, custom functions |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only basic tensors |\n",
        "| IV | Custom nn.Module | Proper subclassing with nn.Parameter |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with manual training loops |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| Standard `nn.Module` | Most deep learning tasks |\n",
        "| Custom `autograd.Function` | Non-differentiable ops, custom gradients |\n",
        "| Custom training loop | GANs, RL, complex multi-model training |\n",
        "| Gradient hooks | Debugging, visualization, per-layer manipulation |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch Part 1** - Fundamentals and high-level API\n",
        "3. **PyTorch Part 2** - Advanced custom components (This notebook!)\n",
        "4. **TensorFlow/Keras** - Alternative framework comparison\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Distributed Training** - Multi-GPU with DDP\n",
        "- **Model Optimization** - TorchScript, quantization, pruning\n",
        "- **PyTorch Lightning** - Even cleaner training code\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}